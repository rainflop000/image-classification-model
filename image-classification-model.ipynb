{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task 1 - Baseline CNN Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load dataset\n",
    "cifar100 = tf.keras.datasets.cifar100\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()\n",
    "\n",
    "# Define model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),  # Input layer\n",
    "    layers.Conv2D(16, (7, 7), activation=\"relu\", padding=\"valid\", strides=1),  # Conv2D layer\n",
    "    layers.MaxPooling2D((2, 2), strides=2),  # MaxPooling2D layer\n",
    "    layers.Conv2D(32, (5, 5), activation=\"relu\", padding=\"valid\", strides=1),  # Conv2D layer\n",
    "    layers.MaxPooling2D((2, 2), strides=2),  # MaxPooling2D layer\n",
    "    layers.Flatten(),  # Flatten layer\n",
    "    layers.Dense(128, activation=\"relu\"),  # Dense layer\n",
    "    layers.Dense(100)  # Output layer\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task 2 - Improve the Baseline Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Load dataset\n",
    "    cifar100 = tf.keras.datasets.cifar100\n",
    "    (train_images, train_labels), (test_images, test_labels) = cifar100.load_data()\n",
    "\n",
    "    # Data augmentation\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomFlip(mode=\"horizontal\", seed=100),\n",
    "        layers.RandomContrast(0.1),\n",
    "    ])\n",
    "\n",
    "    # Define normalization layer\n",
    "    normalization_layer = layers.Normalization(axis=-1)\n",
    "    normalization_layer.adapt(train_images)\n",
    "\n",
    "    # Define model\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    x = normalization_layer(x)\n",
    "\n",
    "    x = layers.Conv2D(24, (3, 3), activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LayerNormalization(axis=-1)(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=2)(x)\n",
    "\n",
    "    x = inception_module(x, [32, 32, 64, 16, 32, 32])\n",
    "\n",
    "    x = layers.Conv2D(64, (4, 4), activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LayerNormalization(axis=-1)(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=2)(x)\n",
    "\n",
    "    x = inception_module(x, [64, 64, 128, 32, 64, 64])\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dense(100)(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(test_images, test_labels))\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "    print(\"\\nTest accuracy:\", test_acc)\n",
    "\n",
    "\n",
    "def inception_module(x, filters):\n",
    "    # Branch 1\n",
    "    branch1x1 = layers.Conv2D(filters[0], (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    # Branch 2\n",
    "    branch3x3 = layers.Conv2D(filters[1], (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    branch3x3 = layers.Conv2D(filters[2], (3, 3), padding=\"same\", activation=\"relu\")(branch3x3)\n",
    "    # Branch 3\n",
    "    branch5x5 = layers.Conv2D(filters[3], (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    branch5x5 = layers.Conv2D(filters[4], (5, 5), padding=\"same\", activation=\"relu\")(branch5x5)\n",
    "    # Branch 4\n",
    "    branch_pool = layers.MaxPooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = layers.Conv2D(filters[5], (1, 1), padding=\"same\", activation=\"relu\")(branch_pool)\n",
    "\n",
    "    # Concatenate branches\n",
    "    x = layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1)\n",
    "    return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Task 3 - Explore Image Classification__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.python.eager.context import PhysicalDevice\n",
    "from typing import List\n",
    "\n",
    "def main():\n",
    "\n",
    "    try:\n",
    "        devices: List[PhysicalDevice] = tf.config.list_physical_devices('GPU')\n",
    "        for device in devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "        print(\"Use devices:\", list(map(lambda d: d.name, devices)))\n",
    "    except IndexError:\n",
    "        print(\"Use CPU\")\n",
    "\n",
    "    # Load dataset\n",
    "    cifar100 = tf.keras.datasets.cifar100\n",
    "    (train_images, train_labels), (test_images, test_labels) = cifar100.load_data()\n",
    "\n",
    "    # Data augmentation\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomFlip(mode=\"horizontal\", seed=100),\n",
    "        layers.RandomContrast(0.1),\n",
    "    ])\n",
    "\n",
    "    # Define normalization layer\n",
    "    normalization_layer = layers.Normalization(axis=-1)\n",
    "    normalization_layer.adapt(train_images)\n",
    "\n",
    "    # Define model\n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    x = normalization_layer(x)\n",
    "\n",
    "    x = layers.Conv2D(24, (3, 3), activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LayerNormalization(axis=-1)(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=2)(x)\n",
    "\n",
    "    x = inception_module(x, [32, 32, 64, 16, 32, 32])\n",
    "\n",
    "    x = layers.Conv2D(64, (4, 4), activation=\"relu\", padding=\"valid\", strides=1)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LayerNormalization(axis=-1)(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=2)(x)\n",
    "\n",
    "    x = inception_module(x, [64, 64, 128, 32, 64, 64])\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dense(100)(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    lr_decay = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        learning_rate, decay_steps=100, decay_rate=0.96, staircase=True)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=lr_decay),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(test_images, test_labels))\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "    print(\"\\nTest accuracy:\", test_acc)\n",
    "\n",
    "\n",
    "# Define Inception module\n",
    "def inception_module(x, filters):\n",
    "    # Branch 1\n",
    "    branch1x1 = layers.Conv2D(filters[0], (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    # Branch 2\n",
    "    branch3x3 = layers.Conv2D(filters[1], (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    branch3x3 = layers.Conv2D(filters[2], (3, 3), padding=\"same\", activation=\"relu\")(branch3x3)\n",
    "    # Branch 3\n",
    "    branch5x5 = layers.Conv2D(filters[3], (1, 1), padding=\"same\", activation=\"relu\")(x)\n",
    "    branch5x5 = layers.Conv2D(filters[4], (5, 5), padding=\"same\", activation=\"relu\")(branch5x5)\n",
    "    # Branch 4\n",
    "    branch_pool = layers.MaxPooling2D((3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "    branch_pool = layers.Conv2D(filters[5], (1, 1), padding=\"same\", activation=\"relu\")(branch_pool)\n",
    "    # Concatenate branches\n",
    "    x = layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1)\n",
    "    return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
